{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3ec657-7c34-47d6-841d-7a427c19f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 가상 환경 media\n",
    "# python = 3.9\n",
    "# pip instal mediapipe\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0fb4f54-4706-464f-a70f-0f03da21b07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이미지 잘 로드되는지 확인 \n",
    "# 구글에서 \"손\" 검색해서 \"손\" 사진 \"hand.jpg\"로 저장\n",
    "import cv2\n",
    "hand = cv2.imread(\"hand.jpg\")\n",
    "cv2.imshow(\"hand\", hand)\n",
    "cv2.waitKey(0) # 무한 대기 -> 아무키를 누르면 다음 코드로넘어감\n",
    "\n",
    "cv2.destroyAllWindows() # 창을 다 끔\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a3bd2e-57bb-4471-aec9-07ff9782144e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv2 추가\n",
    "import cv2\n",
    "\n",
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# solutions 추가\n",
    "from mediapipe import solutions\n",
    "\n",
    "# pb2 추가\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "# numpy 추가\n",
    "import numpy as np\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected hands to visualize.\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "        hand_landmarks = hand_landmarks_list[idx]\n",
    "        handedness = handedness_list[idx]\n",
    "\n",
    "        # Draw the hand landmarks.\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "        ])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks_proto,\n",
    "            solutions.hands.HAND_CONNECTIONS,\n",
    "            solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "            solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        # Get the top left corner of the detected hand's bounding box.\n",
    "        height, width, _ = annotated_image.shape\n",
    "        x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "        y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "        text_x = int(min(x_coordinates) * width)\n",
    "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "        # Draw handedness (left or right hand) on the image.\n",
    "        cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                    (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "# STEP 2: Create an HandLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "                                       num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(\"hand.jpg\")\n",
    "\n",
    "# STEP 4: Detect hand landmarks from the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "# 이미지를 BGR 형식으로 변환\n",
    "image_bgr = cv2.cvtColor(image.numpy_view(),cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# STEP 5: Process the classification result. In this case, visualize it.\n",
    "annotated_image = draw_landmarks_on_image(image_bgr, detection_result)\n",
    "cv2.imshow(\"hand\", cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3104988-905c-4e45-82bb-49bc352c5182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실시간 웹캠으로 손 랜드마크 찍어보자\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "MARGIN = 10\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "        hand_landmarks = hand_landmarks_list[idx]\n",
    "        handedness = handedness_list[idx]\n",
    "\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "        ])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks_proto,\n",
    "            solutions.hands.HAND_CONNECTIONS,\n",
    "            solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "            solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        height, width, _ = annotated_image.shape\n",
    "        x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "        y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "        text_x = int(min(x_coordinates) * width)\n",
    "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "        cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                    (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "                                       num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"웹 캠을 열 수 없습니다\")\n",
    "    exit() # 셀 중지\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: # frame이 들어오지 않으면\n",
    "        break\n",
    "\n",
    "    # 1) frame을 BGR로 변환\n",
    "    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 2) Mediapipe 객체 생성\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_bgr)\n",
    "\n",
    "    # 3) 손 랜드마크 감지\n",
    "    detection_result = detector.detect(mp_image)\n",
    "\n",
    "    # 4) 랜드마크 그리기\n",
    "    annotated_image = draw_landmarks_on_image(frame, detection_result)\n",
    "\n",
    "    # 5) 결과 이미지에 표시\n",
    "    cv2.imshow(\"hand_land\", annotated_image)\n",
    "\n",
    "    # q키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# 종료 시 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07a1a7bf-21f6-48fe-b584-4292c4284998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "100 16990  100 16990    0     0    98k      0 --:--:-- --:--:-- --:--:--   98k\n"
     ]
    }
   ],
   "source": [
    "# 가위, 바위, 보에 대한 정보가 담겨 있는 csv\n",
    "!curl -L https://github.com/kairess/Rock-Paper-Scissors-Machine/raw/refs/heads/main/data/gesture_train.csv -o gesture_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da8c0036-62cd-4c26-b17c-7e1420e931b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 # 웹캠 제어 및 ML 사용 \n",
    "import mediapipe as mp # 손 인식을 할 것\n",
    "import numpy as np\n",
    "\n",
    "max_num_hands = 1 # 손은 최대 1개만 인식\n",
    "gesture = { # **11가지나 되는 제스처 라벨, 각 라벨의 제스처 데이터는 이미 수집됨 (제스처 데이터 == 손가락 관절의 각도, 각각의 라벨)**\n",
    "    0:'fist', 1:'one', 2:'two', 3:'three', 4:'four', 5:'five',\n",
    "    6:'six', 7:'rock', 8:'spiderman', 9:'yeah', 10:'ok',\n",
    "}\n",
    "rps_gesture = {0:'rock', 5:'paper', 9:'scissors'} # 우리가 사용할 제스처 라벨만 가져옴 \n",
    "\n",
    "# MediaPipe hands model\n",
    "mp_hands = mp.solutions.hands # 웹캠 영상에서 손가락 마디와 포인트를 그릴 수 있게 도와주는 유틸리티1\n",
    "mp_drawing = mp.solutions.drawing_utils # 웹캠 영상에서 손가락 마디와 포인트를 그릴 수 있게 도와주는 유틸리티2\n",
    "\n",
    " # 손가락 detection 모듈을 초기화\n",
    "hands = mp_hands.Hands(  \n",
    "    max_num_hands=max_num_hands, # 최대 몇 개의 손을 인식? \n",
    "    min_detection_confidence=0.5, # 0.5로 해두는 게 좋다!  \n",
    "    min_tracking_confidence=0.5)  \n",
    "\n",
    "# 제스처 인식 모델 \n",
    "file = np.genfromtxt('gesture_train.csv', delimiter=',') # **각 제스처들의 라벨과 각도가 저장되어 있음, 정확도를 높이고 싶으면 데이터를 추가해보자!** \n",
    "angle = file[:,:-1].astype(np.float32) # 각도\n",
    "label = file[:, -1].astype(np.float32) # 라벨\n",
    "knn = cv2.ml.KNearest_create() # knn(k-최근접 알고리즘)으로   \n",
    "knn.train(angle, cv2.ml.ROW_SAMPLE, label) # 학습! \n",
    "\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "while cap.isOpened(): # 웹캠에서 한 프레임씩 이미지를 읽어옴\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = hands.process(img)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 각도를 인식하고 제스처를 인식하는 부분 \n",
    "    if result.multi_hand_landmarks is not None: # 만약 손을 인식하면 \n",
    "        for res in result.multi_hand_landmarks: \n",
    "            joint = np.zeros((21, 3)) # joint == 랜드마크에서 빨간 점, joint는 21개가 있고 x,y,z 좌표니까 21,3\n",
    "            for j, lm in enumerate(res.landmark):\n",
    "                joint[j] = [lm.x, lm.y, lm.z] # 각 joint마다 x,y,z 좌표 저장\n",
    "\n",
    "            # Compute angles between joints joint마다 각도 계산 \n",
    "            # **공식문서 들어가보면 각 joint 번호의 인덱스가 나옴**\n",
    "            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19],:] # Parent joint\n",
    "            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],:] # Child joint\n",
    "            v = v2 - v1 # [20,3]관절벡터 \n",
    "            # Normalize v\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis] # 벡터 정규화(크기 1 벡터) = v / 벡터의 크기\n",
    "\n",
    "            # Get angle using arcos of dot product **내적 후 arcos으로 각도를 구해줌** \n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "            angle = np.degrees(angle) # Convert radian to degree\n",
    "\n",
    "            # Inference gesture 학습시킨 제스처 모델에 참조를 한다. \n",
    "            data = np.array([angle], dtype=np.float32)\n",
    "            ret, results, neighbours, dist = knn.findNearest(data, 3) # k가 3일 때 값을 구한다! \n",
    "            idx = int(results[0][0]) # 인덱스를 저장! \n",
    "\n",
    "            # # Draw gesture result\n",
    "            # if idx in rps_gesture.keys(): # 만약 인덱스가 가위바위보 중에 있다면 가위바위보 글씨 표시\n",
    "            #     cv2.putText(img, text=rps_gesture[idx].upper(), org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "            # Other gestures 모든 제스처를 표시한다면 \n",
    "            cv2.putText(img, text=gesture[idx].upper(), org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "            mp_drawing.draw_landmarks(img, res, mp_hands.HAND_CONNECTIONS) # 손에 랜드마크를 그려줌 \n",
    "\n",
    "    cv2.imshow('Game', img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1f5f5-8df7-4f5d-8b9d-a46822fa5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"gesture_train.csv\",\"r\",encoding=\"UTF-8\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a37803-9f9a-406a-b1b0-8fe5f238c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리가 원하는 제스처를 랜드마크 좌표와 클래스를 저장하는 로직\n",
    "\n",
    "# 실시간 웹캠으로 손 랜드마크 찍어보자\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "MARGIN = 10\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "        hand_landmarks = hand_landmarks_list[idx]\n",
    "        handedness = handedness_list[idx]\n",
    "\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "        ])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks_proto,\n",
    "            solutions.hands.HAND_CONNECTIONS,\n",
    "            solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "            solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        height, width, _ = annotated_image.shape\n",
    "        x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "        y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "        text_x = int(min(x_coordinates) * width)\n",
    "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "        cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                    (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "                                       num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# 특정 손 포즈를 취하고\n",
    "# 클래스화 시켜서 랜드마크 좌표랑 같이 저장!!\n",
    "import os\n",
    "csv_file = \"data.csv\"\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "# CSV 파일 헤더 생성\n",
    "header = []\n",
    "for i in range(21):\n",
    "    header.extend([f\"x{i}\", f\"y{i}\", f\"z{i}\"])\n",
    "header.append(\"class\")\n",
    "\n",
    "# a모드는 w모드인데, 파일이 이미 존재하면 덮어쓰기\n",
    "import csv\n",
    "f = open(csv_file, \"a\", newline=\"\")\n",
    "writer = csv.writer(f)\n",
    "\n",
    "# 만약, 헤더가 없다면 헤더를 추가!!\n",
    "# 헤더가 없는 지 어떻게 아냐면... 파일이 없으면 헤더가 없는것!\n",
    "if not file_exists:\n",
    "    writer.writerow(header)\n",
    "\n",
    "print(\"1번키를 누르면 클래스 1로 저장됩니다\")\n",
    "print(\"2번키를 누르면 클래스 2로 저장됩니다\")\n",
    "print(\"3번키를 누르면 클래스 3로 저장됩니다\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"웹 캠을 열 수 없습니다\")\n",
    "    exit() # 셀 중지\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: # frame이 들어오지 않으면\n",
    "        break\n",
    "\n",
    "    # 사용자의 key 입력 감지\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # 1) frame을 BGR로 변환\n",
    "    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 2) Mediapipe 객체 생성\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_bgr)\n",
    "\n",
    "    # 3) 손 랜드마크 감지\n",
    "    detection_result = detector.detect(mp_image)\n",
    "\n",
    "    # 해당 랜드마크를 좌표를 찍어서 csv에 행 추가!!\n",
    "    if detection_result.hand_landmarks: # 손이 감지 되었다면\n",
    "        hand_land = detection_result.hand_landmarks[0]\n",
    "        data = []\n",
    "        for land in hand_land:\n",
    "            data.extend([land.x, land.y, land.z])\n",
    "\n",
    "        if key == ord(\"1\"): # 1번 키를 눌렀다면\n",
    "            data.append(1)\n",
    "            writer.writerow(data)\n",
    "            print(\"클래스 1 저장 완료!!\")\n",
    "        elif key == ord(\"2\"): # 2번 키를 눌렀다면\n",
    "            data.append(2)s\n",
    "            writer.writerow(data)\n",
    "            print(\"클래스 2 저장 완료!!\")\n",
    "        elif key == ord(\"3\"): # 3번 키를 눌렀다면\n",
    "            data.append(3)\n",
    "            writer.writerow(data)\n",
    "            print(\"클래스 3 저장 완료!!\")\n",
    "\n",
    "    # 4) 랜드마크 그리기\n",
    "    annotated_image = draw_landmarks_on_image(frame, detection_result)\n",
    "\n",
    "    # 5) 결과 이미지에 표시\n",
    "    cv2.imshow(\"hand_land\", annotated_image)\n",
    "\n",
    "    # q키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# 종료 시 자원 해제\n",
    "f.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f8aeb-0458-430f-b312-a0acbd51d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 우리 랜드마크 21개의 점 가지고 벡터 연산 해서 gesture 추론 로직\n",
    "# 2) 기존 제스터 모델 사용하려면\n",
    "# -> data.csv(21 x 3) +1 로 되어있는 것을 -> 각 랜드마크 포인트를 각도로 연산(전처리 과정 진행 필요)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
